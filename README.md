


Network-based drug repositioning using
Eigenvector Centrality Measure

A.Ahmadi† ,    K. Kavousi‡*   A.Moeini†,  S. Gharaghani§
†Department Of Algorithms  & Computation,Faculty Of Engineering Sciences,College Of Engineering, University  Of  Tehran, Tehran, Iran. ‡ Laboratory of Complex Biological Systems and Bioinformatics (C.B.B),Institute of Biochemistry and Biophysics (I.B.B),University of Tehran, Tehran, Iran. §Laboratory of Chemoinformatics  & Drug Design (LCD),Institute of Biochemistry and Biophysics,University of Tehran, Tehran, Iran
KEYWORDS : Drug Repositioning, Eigenvector Centrality,  Recommendation Systems
 
ABSTRACT: The design and discovery of a new drugs, is an expensive and time-consuming process. Because of this, we have recently seen a new labeling strategy by drug manufactures. The process attempts “repositioning” by seeking new applications for already existing drugs. Furthermore, the development of various formulations creates new ingredients from the two previous compounds. The substantial privileges of this strategy are: diminished expenses, reduced time-consumption and the lower risk in design and discovery of drug production . 1 Furthermore, according to the expertise, the substituted drugs have the same efficacy and potency compare to the main ones. Thus recently, recommendation methods based on “Network Base Interference” (NBI) have given out such effective outcomes . 2
In this paper, we present a new method based on “Drug-target interaction network” naming “DT-BEIG” in which the concept of “Eigenvector Centrality” can be used in the repositioning. To be more elaborate to the concept of Eigenvector Centrality, we can say the significance of a node increased dependently to its connectivity to the other nodes with the high value and Eigenvector Centrality is a criterion that demonstrates this concept . 3 Therefore, within this perception, by having the knowledge about a node, we are granted access to more information of other nodes. This can’t be achieved through “Centrality by Degree”; the main reason we employed this concept in Drug-target interaction.
Moreover, the predicted algorithm driven from DT-Hybrid has been introduced by Alaimo . 4 This algorithm is based on the last edition of the DTI data set, which has been tested within the Drug Bank organization. Thus, the predicted algorithm could foresee more reliable drug-target interactions in some of data sets compare to DT-Hybrid algorithm.
In the methods based on NBI, if required, the concept of Centrality by Degree has been yield in order to find the significant nodes. However, in this paper; we have used the concept of Eigenvector Centrality in our calculations in regards to the actual repositioning in Drug-target interaction. Consequently, the preferred method in the paper leads us to witness an increase in precision, reliability and prediction relative to the DT-Hybrid algorithm, which also reduced the deviation from the factual scores. These are the remarkable privileges of this application compared to DT-Hybrid method. 


 
Introduction:
The design and discovery of drugs is a phenomenon in which chemical compounds are developed for the purpose of curing diseases. From a historical aspect, the essence of herbal, animal and mineral products has been used for this purpose. With the advancement and development of chemistry in the 19th century, researchers began to isolate the effective ingredients from the essences .5 In 1930 the science of biochemistry, lead to the introduction of enzymes, receptors and their potential capabilities in terms of drug targeting. While the molecular biology, allowed a better conceptualization of the diseases’ mechanism in the molecular level .6
Drug companies who have often relied on more traditional methods of exploring drugs are now looking for new opportunities to modify their processes. In this way, it is hard to meet the medical needs of finding a new appropriate application for many existing drugs or the off market ones due to insufficient information about the traditional drug repositioning mechanisms. Thus, computational methods will be able to reduce these issues, clarifying the unknown mechanisms by combining high level of the existing information .6 Therefore, the computational methods of drug repositioning can be classified as follow: target based, knowledge based, signature based, pathway or network based and target-mechanism based 
.7  These methods focus on various sub-classifications based on the existing information and well known mechanisms such as drug, disease and treatment oriented .7 That is why this method of drug repositioning is more appealing to the researchers in terms of investigating almost all candidate drugs and tests them on relatively numerous diseases in a short amount time frame. 
Moreover, it is significant to know that drugs are effective when they bind to specific proteins. Therefore, any modification in this process will profoundly influence the application of biochemistry and physiology (pharmacodynamics and pharmacokinetics). Due to protein functionality as part of interconnected cellular networks (interactome networks), the “one gene, one drug, one disease” model has been tested in many cases. In this way, the concept of poly-pharmacology employed for the drugs which have multi targets. In spite of this, many interactions are yet unknown and considering the in situ tests, which need numerous sources, it is essential that a series of algorithmic methods be used to make the predictions more feasible for the ongoing process of the new and remarkable interactions for the sake of advanced development. 8
In many literatures, computing tools for the matter of “prediction drug-target interaction and drug repositioning” have been provided. The traditional methods are based on either ligand or receptor. Among the latter, we can use “quantitative structural-activity” connections and a “similarity search” based method . 9 In addition, receptor-based systems, such as “reverts docking”, have also been applied in drug target binding affinity predictions, DTI predication and drug repositioning. Yet, these last ones have the limitation that they cannot be used for targets whose 3D structures are indefinite .10
Lately, more considerations have been dedicated to network based and phenotype-based tactics. Most of them carry on the fruitful opinion of employing “bipartite graphs”. Accordingly   11 and  12  have presented drug repositioning methods, using public gene expression data. Furthermore, 13 developed a bipartite graph, establishing a method to predict DTI, by incorporating chemical and genomic data. 14depicted a technical based on network- based implication NBI applying a simple form of the algorithm suggested by .2 All these outcomes obviously display the virtuous piece of this attitude. However, information about drug and protein domain is not correctly demonstrated. Thus, 15,16  use a machine learning method, beginning from a DTI network, to foresee new ones with high precision. The computation of the new relations is accomplished via the normalized minimum squares algorithm. This algorithm is qualified by means of a GIP (Guassian Interaction Profile) that recapitulates the info in the network. Later, 17announced their “Network-based Random Walk with Restart on the Heterogeneous network” (NRWRH) algorithm forecasting new relations between drugs and targets, by a model based on a casual walk with a restart in a heterogeneous network. This model is built by spreading the network of DTI connections with those of drug-drug and protein-protein similarity networks. This organization showed a brilliant presentation in forecasting new relations. Nevertheless, it suffers from an arbitrary nature, primarily initiated by the first chances of the collection.
18proposed the Bipartite Local Model Interaction-profile Inferring (BLM-NII) algorithm. Interactions among drugs and targets are assumed by preparation classifiers, such as support vector machine or regularized lease square. It accomplishes this by developing interactive information and drug and target similarities. This classifier is correctly prolonged to comprise information on new drug-target candidates, yielding new possibilities for a definite drug. The algorithm is extremely reliable in estimating the connections between new drug-target candidates. While, it also has the ability of preparing numerous diverse classifiers, to attain the concluding model is not durable enough. Also 19presented a method that uses probabilistic matrix factorization (PMF) for this purpose, which is particularly useful for analyzing large interaction networks.
Finally, Alferedo and his colleagues 4 provided a new method based on bipartite graph interaction, which is actually an established method of NBI .20It yields the criteria of the drug similarities and targets for the accomplishment of the computations. But the weak spot of the latter is that it has access to the complete target information in order to predict a new drug.
In this paper, we have recommended a new method of naming DT-Based EIG, which actually is a developed form of the DT-Hybrid algorithm, and have used the Eigenvector Centrality algorithm for prediction. The concept of Eigenvector Centrality is often used in social network and ranking algorithms such as the HITS algorithm and the Eigenvector metric. But it has never been used in repositioning. The results of the computations show that the usage of this concept leads to enhancement of the act of prediction in repositioning. In conclusion, the predicted algorithm can be applied on four data sets .2 derived from the Drug Bank and then compared to the responses with DT-Hybrid. Consequently, the comparisons reveal that the predicted method is much more reliable in both Ion channel and GPCRs.

 Methods:
let  〖X=(x〗_1,x_2,….,x_m)  be a set of small molecules (i.e. biological Compounds), and  T={t_1,t_2,…,t_n} a set of targets (i.e. genes, proteins). the   X-T ، network of interactions can be described as a bipartite graph  G(X,T,E) where E=(e_ij ∶ x_i∈X ,t_j  ∈T). A link between x_iand t_j  is drawn in the graph when the structure x_i is associated with the target t_j. the network can be represented by an adjacent matrix A=〖{a_ij}〗_(n×m) where a_ij=1 if x_iis connected to t_jotherwise a_ij=0
The weight matrix W=〖{w_ij}〗_(n×n)can be computed as follows:
W_ij=1/(Γ(i,j)) ∑_(l=1)^m▒(a_il a_jl)/(K (x_l))                          (1)
Table1 illustrates the Γ function for DT-BEIG and DT-Hybrid algorithms.
k_((x))is the degree of the x  node and   centrality_((x))illustrates the Eigenvector Centrality of the x node in the bipartite network.  According to weight matrix  W and the adjacent matrix A  of the bipartite network, the recommendation matrix R=〖{r_ji}〗_(n×m)  can be computed as follows:
R=W.A                                          (2)
For each x_i in X  ، its recommendation list is given by the set:
R={(t_j,r_ji )│a_ij=0}                    (3)
Where  r_ji is the ‘score’ of recommending t_j to  x_i .This list  is then sorted in to a descending order with respect to the score because the higher elements are expected to have a better interaction with the corresponding structure.

Computing Eigenvector Centrality:
The Eigenvector Centrality values for each nodes are calculated as follows3:
x_i=1/p ∑_(j=1)^n▒〖e_ij y_j  (i=1,…,m) 〗             (4)
y_i=1/p ∑_(j=1)^m▒〖e_ij x_j  (i=1,…,n)              (5)〗
let {γ_1,γ_2,….,γ_m} be the set of the eigenvalues of  B, 
 p=〖max〗_i |γ_i | its spectral radius, if   G=(V,E)  is  undirected, it is well known that  B  is symmetric and its eigenvalues are real . 3
The Perron-Frobenius theorem from linear algebra guarantees that when the network is an undirected, connected component, iterating Eq. (4) and Eq. (5) will always converge on a fixed point equivalent to the principal eigenvector of the adjacency matrix. Thus, we can sidestep the iteration completely and formulate the calculation as an Eigenvector problem of the form:
[■(0&A@A^T&O)]* [█(X@Y)]=P[█(X@Y)]
where  A  is the adjacent matrix,  B=[■(0&A@A^T&O)]  is block adjacency matrix, x  and  y  is a vector containing the eigenvector centralities, and  P  is the spectral radius. 3

Computing Similarity matrix S^((1)):
let S=〖{s_ij}〗_(n×n)be the target similarity matrix, which can be calculated using either BLAST bits scores 21 or Smith-Waterman local alignment scores . 22 Here we have used the normalized version of Smith-Waterman which can be  procedured  as follows i13:
S_norm (i,j)=(S(i,j))/√(S(i,i).S(j,j))                  (6)
S(.,.)   represents main Smith-Waterman score before normalization.
More over S_1=〖{〖S^'〗_ij}〗_(m×m)may be the structure similarity matrix, which can be calculated using the SIMCOMP
similarity score . 23 where SIMCOMP provides a global similarity score based on the size of the common substructures between two compounds using a graph alignment algorithm. The similarity between the two compounds c  and c^' is computed as  S_c (c,c')=  (| c∩c'| )/(|c ∪ c'| ) . 23  It would be possible to calculate the matrix S_2=〖{〖s''〗_ij}〗_(n×n) (where each element 〖s''〗_ijrepresents the similarity between t_iand t_jbased on the common interactions in the network weighted by compound similarity), as follows:
S_ij^''=(∑_(k=1)^m▒∑_(l=1)^m▒〖(a_il a_jk S_lk^')〗)/(∑_(k=1)^m▒∑_(l=1)^m▒〖a_il a_jk 〗)                    (7)
Then, this matrix can be linearly combined with target similarity matrix  S , as follows:
S^((1))=αS+(1-α) S_2                         (8)
Eventually S^((1)) matrix can be used in the   function.
α and  λ values are dependent on the type of data sets experimentally which can be achieved on the basis of repeated experiments. Values for each data set are specified in Table 2.

Datasets and benchmarks:
We  evaluated our method using four datasets containing Enzyme, GPCR, Ion channel and Nuclear Receptor and to perform the repositioning operation, we tested our algorithm on Complete Drugbank data set (Including approved and experimental drugs).
the datasets which provided by  2 are publicly available at http://www.lmmd.org/database/dti/. Table 3  consisting the lists with some properties of the datasets. also, all DTI data in the benchmark data sets were collected from KEGG BRITE, BRENDA, SuperTarget and DrugBank . 2

To test the performance of the methods, 10-fold cross-validation approach was applied and each result was yielded by recalculating 30 times. For each data set, all the DTIs were randomly divided into 10 parts with equal size. Each part was taken in turn as the test set,  while the remaining nine parts were served as the training set. With the randomly splitting, some targets (or drugs) may be just in the test set and the corresponding links without any information in the training set could not be predicted with the DT-BEIG method. Such links were not considered in the performance assessment. so that for each node, at least one link to the other nodes remains in the test set.
To do so, we consider 1/10th the existing interactions in the dataset for each train set. In the way that in each time software application, in order to get the data set, relative to the number of the links in the graph (the numbers of 1’s in the adjacent matrix), we choose 1/10th of the present link randomly and equalize their amount into zero in the adjacent matrix. Then, we apply the same process for another 9 train data set in the next procedures. It is significant not to neglect the fact that the chosen links for deletion in train data sets must not be equal as far as possible.

Evaluation:
In order to compare the performance of the methods, the following two metrics were considered: precision and recall enhancement.4
Precision and recall enhancement, e_P (L)and e_R (L): a more practical measure may be to consider d_i (L), the number of Drug i’s deleted interactions
contained in the top  L  places . we may be interested either in  how many of these top  L places are occupied by deleted links, or how many of the Drug’s D_ideleted interactions have been recovered in this way. Averaging these ratios  (d_i (L))/L and (d_i (L))/D_i  over all Drugs with at least one deleted link, we obtain the mean precision and recall, P(L)  and  R(L)  of the recommendation process:
P(L)=1/m' ∑_(i=1)^m'▒(D_i (L))/L                         (9)
R(L)=1/m' ∑_(i=1)^m'▒(D_i (L))/D_i                            (10)
We have taken into account that the value of  L is equal to 20, where  m'  is the number of structures with at least one deleted link. A still better perspective may be given by considering these values relative to the precision and recall of random recommendations 〖  P〗_rand (L)  and  R_rand (L). 6   If drug  i  as a total of  D_ideleted interactions, then   P_rand^i (L)=D_i/(n-K(i))≈D_i/n(since in general n≫ K_i),and hence averaging over all  drugs,〖 P〗_rand (L)=D/nm where  D  is the total number of deleted interactions. By contrast the mean number of deleted interactions in the top  L  places is given by (LD_i)/((n-K_i))≈(L D_i)/n  and  so R_rand (L)=  L/n. From this we can define the precision and recall   enhancement : 20 
e_P (L)=(P(L))/(P_rand (L))=P(L).(n.m)/D                        (11)
e_R (L)=(R(L))/(R_rand (L))=R(L).n/L                             (12)
The third measure for evaluating the method in this paper, in addition to Mentioned metrics, is Mean Absolute Error (MAE) which computes the deviation between predicted ratings and actual ratings and can be calculated as  fallows : 24
MAE=1/n  ∑_(i=1)^n▒〖|p_i-r_i |                   (13)〗
We assume that p_iis a real score, r_iis a predicted one and  n  is the number of omitted links  i. We actually do the prediction in order to find the amount of p_i without deleting any link form the data set which means that we do the prediction on the data sets which interactions among them are distinguished previously and the present score is the best expected one. Now, r_i reached by omitting a series of interactions and the software should restore the deleted links and calculate definitive score  (r_i ).  Now, the closer the amount of  r_ito p_i.the lesser is the amount of deviation in the act of prediction.

Results:
In this paper, we presented a developed form of an algorithm from DT-Hybrid to DT-BEIG. In the way that we calculate the amount of centrality based on the specific vector of all nodes within the bipartite graph (drugs and targets) and yield those for the calculation of edge’s weight of the graph while in DT-Hybrid algorithm only the concept of Degree Centrality is being used. The tests prove that the usage of this method enhance the precision in prediction in the data sets of GPCR, Ion channels and complete DrugBank. ( figure 1  shows the results) and also, to promote ROC curve on the two data sets of GPCR, Ion channels and the Complete DrugBank ( figure 2  and 3  show the results).
Moreover, computing the MAE criterion on the four data sets reveals that the amount of the score deviation from the actual one in the recommended algorithm compare to DT-Hybrid in the data sets of GPCR, Nuclear receptor and Ion channel is much inferior relatively as the results of computations of MAE criterion on the datasets are shown in the table 4. 
The main reason which the predicted algorithm would not be able to give out a better result on the data sets is that the Hub in the network among the nodes within the bipartite graph is distributed unequally and Enzyme Conservation Score. Since we plot the network graph of the Enzyme by means of  Cytoscape software and distinguishing the 20 Hubs within it, we noticed that among the 20 achieved Hubs only 2 of them belong to the target nodes and the remaining fit to the drug nodes. The Hubs regarding to GPCR, Ion channel and Nuclear receptor have been also plotted which are brought in table 5.  It is clear that Hub in the networks of Ion channel, GPCR and Nuclear receptor with considering its measure of the network in between the target and drug nodes has such even distribution while this statement doesn’t follow in Enzyme’s network. Thus, the strategic practices of Eigenvector Centrality don’t give out the satisfactory outcomes.
When the sequences of a given protein are compared between taxa  .using multiple sequence alignment (MSA), differences between sequences most often represent mutations that were allowed (by evolution) to persist because they were harmless. Where the sequences are identical, we say that sequence was conserved. Such evolutionary conservation occurs because mutations of these amino acids were harmful to protein function, and were lost over time. Amino acids that are conserved are those most critical to the function of   the   protein .25
Now, to compute the conservation of the proteins sequence (enzymes, Ion channel, GPCR, Nuclear receptor) we have used web server of “ConSeq”.26
ConSeq (http://conseq.tau.ac.il) are one well established web server for calculating the evolutionary conservation of amino acid positions in proteins using  an  empirical  Bayesian  inference ,starting from protein sequence . 26
A flowchart of the computing Conservation is shown in Figure 4   and detailed below:
	All sequence is extracted from the Kegg database (http://www.genome.jp/).
	Multiple sequence alignment are calculated using the CLUSTALW or MUSCLE program (http://www.ebi.ac.uk/services). The user is requested to paste the query sequence of the protein in a FASTA format.
	builds a phylogenetic tree consistent with the MSA.
	calculates the conservation scores using either an empirical Bayesian or the Maximum Likelihood method.

The results in the computation of conservation score are shown in  the  table 6  . With consideration to the concept of conservation, the more conservation score of the protein dataset is, the more similar sequences of amino acids’ role in the protein structure and the more similarity, the better result of drug-target interaction prediction in regards to its precision can take place because according to the mentioned method of this article, the prediction highly dependents to the extent of proteins similarity and drugs as well.
In order to validate the predictions experimentally, one enzyme, DPP-IV, and six receptors,   ER α,  ERβ,  β_1 adrenergic   α_1Aadrenergic,  α_1Badrenergic  and  HistamineH_4, were selected as the targets, just because the drug screening systems of these targets are available in our  laborator . 23  By applying the proposed algorithm on the Complete  drugbank data set, and the repeated 30 times, all new potential drugs with  DPP-IV ,  ER-α,  ER-β, 〖 β〗_1 adrenergic , α_1Aadrenergic, α_1B  adrenergic  and  Histamine H_4  targets, were predicted with a better score which represents high reliability in the prediction operation. Prediction results, and scores are shown in Figure 5.

Conclusions:
We presented a method in this paper based on network (including drug and target similarity) and centrality concepts based on specific vectors for the purpose of predicting DIT, have been taken place. The degree of a node is only a local criterion, which distinguishes the level of its significance. In the case of developing the centrality based on the degree, we may say that the importance of a node is increased when it connects to the higher and more valuable nodes. Thus, centrality based on specific axis is a criterion that states this concept. According to the concept and knowledge about a node we will be able to access the other nodes’ information. However, this achievement won’t take place through the centrality of a node. Therefore, retaining the concept of centrality based on a specified vector will improve the predicted results of the computations in the two data sets of Ion channel and GPCR relative to DT-Hybrid.

Yielding the concept of Eigenvector Centrality often is common in social networks, ranking algorithms such as HITS algorithm and Eigen-factor metric yet it hasn’t been engaged in the practice of repositioning 27 Last but not least, the results show that in the case of equal distribution among hubs in the biological networks, the usage of this concept lead to a much more fruitful prediction in repositioning. 

The maximum amount of precision and recall depended on the amount of L and the number of interactions omitted. After implementation of the algorithm, we computed the maximum amount of precision and recall by counting the number of omitted interactions in each time of software application.

A question has emerged regarding the amount of precision and recall mentioned in the inclusion segments and that is why the amount of Precision is less when compared to its maximum amount, while no such notable discrepancy can be seen in the amounts of Recall. The solution depends on the amount of fp and tp. If the amount of fp turns to 1, the algorithm has recovered the wrong link instead of the right one. Also, if the amount of fn turns to 1, the algorithm has recovered and introduced the right link as the wrong one. We may interpret that, here, the algorithm has taken the chance of prediction of the right link from the software. 

As it is obvious, for computation of Precision and Recall, the amounts of fp and fn should be considered respectively. The reduction in the amount of Precision means the increase number of fp and increase amount of Recall shows reduction in number of fn. Now it is important to consider that the risk of the increase number of fn relative to the increase number of fp is more because the increase number of fn relatively means that we introduce lots of wrong links as the right ones. For instance, if a drug company spends the time and expense to verify the results from the algorithm based on the wrong predication, all the time and expenses are fully wasted. While, the increase number of fn relatively means that we have missed the prediction and finding the right links which could potentially be proven for drug-targeting purposes after clinical laboratorial tests and taken the opportunity of introducing the new drugs from drugs company. Thus, it is definite that the increase number of fp is relatively fortunate than the increase number of fn while, according to the researches, the real amount of Precision in Imbalance data is reduced .28 

AUTHOR INFORMATION
Corresponding Author
* Email:kkavousi@yahoo.com

REFERENCES
(1) Sekhon, B. S. Repositioning drugs and biologics: Retargeting old/existing drugs for potential new therapeutic applications .J. Pharm. Ed. Res. 2013, 4, 1–15.
(2) Cheng, F.; Liu, C.; Jiang, J.; Lu, W.; Li, W.; Liu, G.; Zhou, W.; Huang, J.; Tang, Y. Prediction of drug-target interactions and  Drug repositioning via Network-based inference .PLoS computational biology .2012, 8, e1002503.
 (3) Grassi, R.; Stefani, S.; Torriero, A. Using bipartite graphs to assess power in organizational networks: a case study. Dynamics of Socioeconomic Systems, DYSES Jour-nal . 2011,
(4) Alaimo, S.; Pulvirenti, A.; Giugno, R.; Ferro, A. Drug--target interaction prediction through domain-tuned network-based inference .Bioinformatics . 2013, 29, 2004–2008.
(5) Li, Y. Y. Bioinformatic approaches to drug repositioning. University of British Columbia . 2011,
(6) Drews, J. Drug discovery: a historical perspective .Science  2000, 287, 1960–1964.
(7) Iorio, F.; Bosotti, R.; Scacheri, E.; Belcastro, V.; Mithbaokar, P.; Ferriero, R.;Murino, L.; Tagliaferri, R.; Brunetti-Pierri, N.; Isacchi, A. Discovery of drug mode of action and drug repositioning from transcriptional responses .Proceedings  of  the  National  Academy  of  Sciences . 2010, 107, 14621–14626.
(8) Hopkins, A. L. Network pharmacology: the next paradigm in drug discovery .Nature chemical biology. 2008, 4, 682–690.
(9) González-Díaz, H.; Prado-Prado, F.; García-Mera, X.; Alonso, N.; Abeijón, P.; Caa-mano, O.; Yanez, M.; Munteanu, C. R.; Pazos, A.; Dea-Ayuela, M. A.  Mind-best: web server for drugs and target discovery; design, synthesis, and assay of mao-b inhibitors and theoretical- experimental study of g3pdh protein from trichomonas gallinae .Journal   of   proteome   research   . 2011, 10, 1698–1718.
(10) Xie, L.; Evangelidis, T.; Xie, L.; Bourne, P. E. Drug discovery using chemical systems biology: weak inhibition of multiple kinases may contribute to the anti-cancer effect of nelfinavir .PLoS  computational biology . 2011, 7, e1002037.
(11) Sirota, M.; Dudley, J. T.; Kim, J.; Chiang, A. P.; Morgan, A. A.; Sweet-Cordero, A.; Sage, J.; Butte, A. J. Discovery and preclinical validation of drug indications using compendia of public gene expression data .Science  translational  medicine. 2011, 3, 96ra77–96ra77.
(12) Dudley, J. T.; Sirota, M.; Shenoy, M.; Pai, R. K.; Roedder, S.; Chiang, A. P.; Mor-gan, A. A.; Sarwal, M. M.; Pasricha, P. J.; Butte, A. J. Computational repositioning of the anticonvulsant topiramate for inflammatory bowel disease. Science  translational  medicine.  2011, 3, 96ra76–96ra76.
(13) Yamanishi, Y.; Araki, M.; Gutteridge, A.; Honda, W.; Kanehisa, M. Prediction of drug--target interaction networks from the integration of chemical and genomic spaces .Bioinformatics .2008, 24, i232–i240.
(14) Zhou, T.; Ren, J.; Medo, M.; Zhang, Y.-C. Bipartite network projection and personal recommendation .Physical Review E 2007, 76, 046115.
(15) Van Laarhoven, T.; Nabuurs, S. B.; Marchiori, E. Gaussian interaction profile kernels for predicting drug--target interaction .Bioinformatics.  2011, 27, 3036–3043.
(16) Sugaya, N. Training based on ligand efficiency improves prediction of bioactivities of ligands and drug target proteins in a machine learning approach .  Journal of Chemical Information and Modeling .2013, 53, 2525–2537.
(17) Chen, X.; Liu, M.-X.; Yan, G.-Y. Drug--target interaction prediction by random walk on the heterogeneous network Molecular. BioSystems.  2012, 8, 1970–1978.
(18) Mei, J.-P.; Kwoh, C.-K.; Yang, P.; Li, X.-L.; Zheng, J. Drug--target interaction prediction by learning from local information and neighbors .Bioinformatics.  2013, 29, 238–245.
(19) Cobanoglu, M. C.; Liu, C.; Hu, F.; Oltvai, Z. N.; Bahar, I. Predicting drug--target interactions using probabilistic matrix factorization. Journal of Chemical Information and Modeling .2013, 53, 3399–3409.
(20) Zhou, T.; Kuscsik, Z.; Liu, J.-G.; Medo, M.; Wakeling, J. R.; Zhang, Y.-C. Solving the apparent diversity-accuracy dilemma of recommender systems .Proceedings  of the National Academy of Sciences .2010, 107, 4511–4515.
(21) Altschul, S. F.; Gish, W.; Miller, W.; Myers, E. W.; Lipman, D. J. Basic local alignment search tool.Journal of molecular biology. 1990, 215, 403–410.
(22) Smith, T. F.; Waterman, M. S. {I}dentification of common molecular subsequencesl .Journal of molecular biology .1981, 147, 195–197.
(23) Hattori, M.; Okuno, Y.; Goto, S.; Kanehisa, M. Development of a chemical structure comparison method for integrated analysis of chemical and genomic information in the metabolic pathways .Journal of the American Chemical Society. 2003, 125, 11853–11865.
  (24) Massa, P., Avesani, P., Eds. Trust metrics in recommender systems, 3rd ed.; Trust metrics in recommender systems. Springer:New York,.2009.
(25) Bejerano, G.; Pheasant, M.; Makunin, I.; Stephen, S.; Kent, W. J.; Mattick, J. S.;Haussler, D. Ultraconserved elements in the human genome .Science. 2004, 304, 1321–1325.
(26) Ashkenazy, H.; Erez, E.; Martz, E.; Pupko, T.; Ben-Tal, N. ConSurf 2010: Calculating evolutionary conservation in sequence and structure of proteins and nucleic acids Nucleic acids research  .2010,
(27) Martin, T.; Zhang, X.; Newman, M. Localization and centrality in networks. arXiv preprint arXiv:1401.5093 2014,
(28) Avazpour, I.; Pitakrat, T.; Grunske, L.; Grundy, J. Dimensions and metrics for evalu-ating recommendation systems; 2014.
 

